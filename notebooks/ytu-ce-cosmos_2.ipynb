{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New text:  \n",
      "Tabelaları çok küçük renksiz ışıksız Gerçekten anlayamıyoruz [pp] (fdfd)\n",
      "\n",
      "{'entity': 'non', 'score': 0.9999956, 'index': 1, 'word': 'Tabelaları', 'start': 1, 'end': 11}\n",
      "{'entity': 'non', 'score': 1.0, 'index': 2, 'word': 'çok', 'start': 12, 'end': 15}\n",
      "{'entity': ',', 'score': 0.99981433, 'index': 3, 'word': 'küçük', 'start': 16, 'end': 21}\n",
      "{'entity': 'non', 'score': 1.0, 'index': 4, 'word': 'renk', 'start': 22, 'end': 26}\n",
      "{'entity': ',', 'score': 0.9999659, 'index': 5, 'word': '##siz', 'start': 26, 'end': 29}\n",
      "{'entity': 'non', 'score': 1.0, 'index': 6, 'word': 'ışık', 'start': 30, 'end': 34}\n",
      "{'entity': 'non', 'score': 0.9354928, 'index': 7, 'word': '##sız', 'start': 34, 'end': 37}\n",
      "{'entity': 'non', 'score': 0.9999982, 'index': 8, 'word': 'Gerçekten', 'start': 38, 'end': 47}\n",
      "{'entity': 'non', 'score': 1.0, 'index': 9, 'word': 'anlaya', 'start': 48, 'end': 54}\n",
      "{'entity': 'non', 'score': 0.6378202, 'index': 10, 'word': '##mıyoruz', 'start': 54, 'end': 61}\n",
      "{'entity': 'non', 'score': 1.0, 'index': 11, 'word': '[', 'start': 62, 'end': 63}\n",
      "{'entity': 'non', 'score': 1.0, 'index': 12, 'word': 'pp', 'start': 63, 'end': 65}\n",
      "{'entity': 'non', 'score': 1.0, 'index': 13, 'word': ']', 'start': 65, 'end': 66}\n",
      "{'entity': 'non', 'score': 1.0, 'index': 14, 'word': '(', 'start': 67, 'end': 68}\n",
      "{'entity': 'non', 'score': 1.0, 'index': 15, 'word': 'f', 'start': 68, 'end': 69}\n",
      "{'entity': 'non', 'score': 1.0, 'index': 16, 'word': '##df', 'start': 69, 'end': 71}\n",
      "{'entity': 'non', 'score': 1.0, 'index': 17, 'word': '##d', 'start': 71, 'end': 72}\n",
      "{'entity': 'non', 'score': 0.9999999, 'index': 18, 'word': ')', 'start': 72, 'end': 73}\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, BertForTokenClassification\n",
    "\n",
    "noktalama_isaretleri = ['!', '?', '.', ',', '-', ':', ';', \"'\"]\n",
    "\n",
    "def preprocess(text):\n",
    "    # Define punctuation marks to keep\n",
    "    noktalama_isaretleri = ['!', '?', '.', ',', '-', ':', ';', \"'\"]\n",
    "    # Remove punctuation marks defined in noktalama_isaretleri\n",
    "    new_text = ''.join(char for char in text if char not in noktalama_isaretleri)\n",
    "    print(\"New text: \", new_text)\n",
    "    \n",
    "    # # First pass: Keep alphanumeric, spaces and punctuation marks\n",
    "    # new_text = \"\"\n",
    "    # for char in text:\n",
    "    #     if (char in noktalama_isaretleri or \n",
    "    #         char.isalnum() or \n",
    "    #         char.isspace()):\n",
    "    #         new_text += char\n",
    "            \n",
    "    # # Second pass: Keep only alphanumeric, spaces and specific marks\n",
    "    # new_text_pure = \"\"\n",
    "    # for char in text:\n",
    "    #     if (char.isalnum() or \n",
    "    #         char.isspace() or \n",
    "    #         char in [\"'\", \"-\"]):\n",
    "    #         new_text_pure += char\n",
    "            \n",
    "    # # Replace special characters with spaces\n",
    "    # new_text_pure = new_text_pure.replace(\"'\", \" \")\n",
    "    # new_text_pure = new_text_pure.replace(\"-\", \" \")\n",
    "    \n",
    "    # # Convert to lowercase and handle Turkish characters\n",
    "    # new_text = new_text_pure.replace(\"I\", \"ı\").lower()\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def punctate(sent, punc_corr):\n",
    "    sent = preprocess(sent)\n",
    "    r2 = punc_corr(sent)\n",
    "    for r in r2:\n",
    "        print(r)\n",
    "    tokenized_sent = tokenizer.tokenize(sent)\n",
    "    final_sent = ''\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < len(tokenized_sent):\n",
    "        token = tokenized_sent[i]\n",
    "\n",
    "        if r2[i]['entity'] != 'non':\n",
    "            token += r2[i]['entity']\n",
    "\n",
    "        if r2[i]['entity'] not in [\"'\", \"-\"]:\n",
    "            token += ' '\n",
    "\n",
    "        final_sent += token\n",
    "        i += 1\n",
    "\n",
    "    final_sent = final_sent.replace(' ##', '')\n",
    "\n",
    "    return final_sent\n",
    "\n",
    "\n",
    "def end2end(sent, capitalization_corr, punc_corr):\n",
    "\n",
    "    p_sent = preprocess(sent)\n",
    "\n",
    "    r1 = capitalization_corr(p_sent)\n",
    "    r2 = punc_corr(p_sent)\n",
    "    print(\"Punc: \", r2)\n",
    "\n",
    "    tokenized_sent = tokenizer.tokenize(p_sent)\n",
    "\n",
    "    final_sent = ''\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < len(tokenized_sent):\n",
    "        token = tokenized_sent[i]\n",
    "        if r1[i]['entity'] == 'one':\n",
    "            token = token.capitalize()\n",
    "        elif r1[i]['entity'] == 'cap':\n",
    "            token = token.upper()\n",
    "            while tokenized_sent[i+1].startswith(\"##\"):\n",
    "                token += tokenized_sent[i+1][2:].upper()\n",
    "                i += 1\n",
    "\n",
    "        if r2[i]['entity'] != 'non':\n",
    "            token += r2[i]['entity']\n",
    "\n",
    "        if r2[i]['entity'] not in [\"'\", \"-\"]:\n",
    "            token += ' '\n",
    "\n",
    "        final_sent += token\n",
    "        i += 1\n",
    "\n",
    "    final_sent = final_sent.replace(' ##', '')\n",
    "\n",
    "    return final_sent\n",
    "\n",
    "\n",
    "cap_model = BertForTokenClassification.from_pretrained(\n",
    "    \"ytu-ce-cosmos/turkish-base-bert-capitalization-correction\")\n",
    "punc_model = BertForTokenClassification.from_pretrained(\n",
    "    \"ytu-ce-cosmos/turkish-base-bert-punctuation-correction\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"ytu-ce-cosmos/turkish-base-bert-capitalization-correction\")\n",
    "\n",
    "\n",
    "capitalization_corr = pipeline(\"ner\", model=cap_model, tokenizer=tokenizer)\n",
    "punc_corr = pipeline(\"ner\", model=punc_model, tokenizer=tokenizer)\n",
    "\n",
    "sent = \"\"\"\n",
    "Tabelaları çok küçük, renksiz, ışıksız. Gerçekten anlayamıyoruz [pp] (fdfd).\n",
    "\"\"\"\n",
    "\n",
    "# print(end2end(sent, capitalization_corr, punc_corr))\n",
    "res = punctate(sent, punc_corr)\n",
    "# Geçen hafta sonu arkadaşlarımla birlikte kısa bir tatile çıktık. Cuma akşamı yola çıktık. Yolculuk oldukça keyifli geçti. Cumartesi sabahı otele vardık. Odalarımıza yerleştikten sonra kahvaltıya indik. Kahvaltıda birçok seçenek vardı; omlet, simit, taze sıkılmış portakal suyu ve çeşitli peynirler. Kahvaltıdan sonra sahile gitmeye karar verdik. Deniz çok sakindi ve hava mükemmeldi. Denizde yüzdük, kumda yürüdük ve güneşlendik. Öğleden sonra şehri gezmeye çıktık. Tarihi yerleri ziyaret ettik ve bol bol fotoğraf çektik. Akşam yemeği için meşhur bir restorana gittik. Deniz ürünleri gerçekten çok tazeydi. Yemek sonrası otele döndüğümüzde çok yorgunduk ama tatilin ilk günü harika geçmişti. Pazar sabahı erken kalkıp bir doğa yürüyüşüne çıktık. Orman içinde yürümek çok huzur vericiydi. Dönüş yolunda biraz trafik vardı ama bu güzel tatilin ardından hiçbiri moralimizi bozamazdı. Eve vardığımızda herkes mutlu ve huzurluydu. Bir sonraki tatili planlamaya başladık bile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dördüncü dersimiz arkadaşlar ve eğlenceyi hayatından çıkarma. \n"
     ]
    }
   ],
   "source": [
    "sent = \"\"\"\n",
    "Dördüncü dersimiz, arkadaşlar ve eğlenceyi hayatından çıkarma\n",
    "\"\"\"\n",
    "\n",
    "print(end2end(sent, capitalization_corr, punc_corr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
