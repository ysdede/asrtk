{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'non', 'score': 1.0, 'index': 1, 'word': 'Çünkü', 'start': 1, 'end': 6}, {'entity': 'non', 'score': 1.0, 'index': 2, 'word': 'benim', 'start': 7, 'end': 12}, {'entity': 'non', 'score': 1.0, 'index': 3, 'word': 'geldiğim', 'start': 13, 'end': 21}, {'entity': 'non', 'score': 1.0, 'index': 4, 'word': 'durağı', 'start': 22, 'end': 28}, {'entity': 'non', 'score': 1.0, 'index': 5, 'word': 'görebil', 'start': 29, 'end': 36}, {'entity': 'non', 'score': 1.0, 'index': 6, 'word': '##me', 'start': 36, 'end': 38}, {'entity': 'non', 'score': 1.0, 'index': 7, 'word': 'şansım', 'start': 39, 'end': 45}, {'entity': 'non', 'score': 1.0, 'index': 8, 'word': 'yok', 'start': 46, 'end': 49}, {'entity': '.', 'score': 0.9999964, 'index': 9, 'word': '.', 'start': 49, 'end': 50}, {'entity': 'non', 'score': 1.0, 'index': 10, 'word': 'Konum', 'start': 51, 'end': 56}, {'entity': 'non', 'score': 1.0, 'index': 11, 'word': 'bazen', 'start': 57, 'end': 62}, {'entity': 'non', 'score': 1.0, 'index': 12, 'word': 'yetiş', 'start': 63, 'end': 68}, {'entity': 'non', 'score': 1.0, 'index': 13, 'word': '##emiyor', 'start': 68, 'end': 74}, {'entity': ',', 'score': 0.9523529, 'index': 14, 'word': ',', 'start': 74, 'end': 75}, {'entity': 'non', 'score': 1.0, 'index': 15, 'word': 'yanlış', 'start': 76, 'end': 82}, {'entity': 'non', 'score': 1.0, 'index': 16, 'word': 'oluyor', 'start': 83, 'end': 89}, {'entity': 'non', 'score': 0.8819508, 'index': 17, 'word': ',', 'start': 89, 'end': 90}, {'entity': 'non', 'score': 1.0, 'index': 18, 'word': 'şoför', 'start': 91, 'end': 96}, {'entity': 'non', 'score': 1.0, 'index': 19, 'word': '##e', 'start': 96, 'end': 97}, {'entity': 'non', 'score': 1.0, 'index': 20, 'word': 'sürekli', 'start': 98, 'end': 105}, {'entity': 'non', 'score': 1.0, 'index': 21, 'word': 'sorma', 'start': 106, 'end': 111}, {'entity': 'non', 'score': 1.0, 'index': 22, 'word': '##nı', 'start': 111, 'end': 113}, {'entity': 'non', 'score': 1.0, 'index': 23, 'word': '##z', 'start': 113, 'end': 114}, {'entity': 'non', 'score': 1.0, 'index': 24, 'word': 'gerekiyor', 'start': 115, 'end': 124}, {'entity': '.', 'score': 0.9999924, 'index': 25, 'word': '.', 'start': 124, 'end': 125}, {'entity': 'non', 'score': 1.0, 'index': 26, 'word': 'Halbuki', 'start': 126, 'end': 133}, {'entity': 'non', 'score': 1.0, 'index': 27, 'word': 'sesli', 'start': 134, 'end': 139}, {'entity': 'non', 'score': 1.0, 'index': 28, 'word': 'sistem', 'start': 140, 'end': 146}, {'entity': 'non', 'score': 1.0, 'index': 29, 'word': 'düzgün', 'start': 147, 'end': 153}, {'entity': 'non', 'score': 1.0, 'index': 30, 'word': 'çalış', 'start': 154, 'end': 159}, {'entity': 'non', 'score': 0.99999976, 'index': 31, 'word': '##sa', 'start': 159, 'end': 161}, {'entity': 'non', 'score': 1.0, 'index': 32, 'word': 'hiçbir', 'start': 162, 'end': 168}, {'entity': 'non', 'score': 1.0, 'index': 33, 'word': 'problem', 'start': 169, 'end': 176}, {'entity': 'non', 'score': 1.0, 'index': 34, 'word': 'olmayacak', 'start': 177, 'end': 186}, {'entity': '.', 'score': 0.99999547, 'index': 35, 'word': '.', 'start': 186, 'end': 187}, {'entity': 'non', 'score': 1.0, 'index': 36, 'word': 'Veya', 'start': 188, 'end': 192}, {'entity': 'non', 'score': 1.0, 'index': 37, 'word': 'minibüs', 'start': 193, 'end': 200}, {'entity': 'non', 'score': 1.0, 'index': 38, 'word': '##lerin', 'start': 200, 'end': 205}, {'entity': 'non', 'score': 1.0, 'index': 39, 'word': 'tabela', 'start': 206, 'end': 212}, {'entity': 'non', 'score': 1.0, 'index': 40, 'word': '##ları', 'start': 212, 'end': 216}, {'entity': 'non', 'score': 1.0, 'index': 41, 'word': 'çok', 'start': 217, 'end': 220}, {'entity': 'non', 'score': 1.0, 'index': 42, 'word': 'küçük', 'start': 221, 'end': 226}, {'entity': 'non', 'score': 0.8151957, 'index': 43, 'word': ',', 'start': 226, 'end': 227}, {'entity': 'non', 'score': 1.0, 'index': 44, 'word': 'renk', 'start': 228, 'end': 232}, {'entity': 'non', 'score': 1.0, 'index': 45, 'word': '##siz', 'start': 232, 'end': 235}, {'entity': 'non', 'score': 0.9974964, 'index': 46, 'word': ',', 'start': 235, 'end': 236}, {'entity': 'non', 'score': 1.0, 'index': 47, 'word': 'ışık', 'start': 237, 'end': 241}, {'entity': 'non', 'score': 1.0, 'index': 48, 'word': '##sız', 'start': 241, 'end': 244}, {'entity': '.', 'score': 0.9998808, 'index': 49, 'word': '.', 'start': 244, 'end': 245}, {'entity': 'non', 'score': 1.0, 'index': 50, 'word': 'Gerçekten', 'start': 246, 'end': 255}, {'entity': 'non', 'score': 1.0, 'index': 51, 'word': 'anlaya', 'start': 256, 'end': 262}, {'entity': 'non', 'score': 1.0, 'index': 52, 'word': '##mıyoruz', 'start': 262, 'end': 269}, {'entity': '.', 'score': 0.99997747, 'index': 53, 'word': '.', 'start': 269, 'end': 270}]\n",
      "[UNK] Benim geldiğim durağı görebilme şansım yok .. [UNK] Bazen yetişemiyor ,, yanlış oluyor , şoföre sürekli sormanız gerekiyor .. [UNK] Sesli sistem düzgün çalışsa hiçbir problem olmayacak .. [UNK] Minibüslerin tabelaları çok küçük , renksiz , ışıksız .. [UNK] Anlayamıyoruz .. \n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, BertForTokenClassification\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    noktalama_isaretleri = ['!', '?', '.', ',', '-', ':', ';', \"'\", '[', ']']\n",
    "    new_text = \"\".join(\n",
    "        [char for char in text if char in noktalama_isaretleri or char.isalnum() or char.isspace()])\n",
    "    new_text_Pure = \"\".join([char for char in text if char.isalnum(\n",
    "    ) or char.isspace() or char == \"'\" or char == \"-\"])\n",
    "    new_text_Pure = new_text_Pure.replace(\"'\", \" \").replace(\"-\", \" \")\n",
    "    new_text = new_text_Pure.replace(\"I\", \"ı\").lower()\n",
    "    print(new_text)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def end2end(sent, capitalization_corr, punc_corr):\n",
    "\n",
    "    p_sent = preprocess(sent)\n",
    "\n",
    "    r1 = capitalization_corr(p_sent)\n",
    "    r2 = punc_corr(p_sent)\n",
    "    print(r2)\n",
    "\n",
    "    tokenized_sent = tokenizer.tokenize(p_sent)\n",
    "\n",
    "    final_sent = ''\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    while i < len(tokenized_sent):\n",
    "        token = tokenized_sent[i]\n",
    "        if r1[i]['entity'] == 'one':\n",
    "            token = token.capitalize()\n",
    "        elif r1[i]['entity'] == 'cap':\n",
    "            token = token.upper()\n",
    "            while tokenized_sent[i+1].startswith(\"##\"):\n",
    "                token += tokenized_sent[i+1][2:].upper()\n",
    "                i += 1\n",
    "\n",
    "        if r2[i]['entity'] != 'non':\n",
    "            token += r2[i]['entity']\n",
    "\n",
    "        if r2[i]['entity'] not in [\"'\", \"-\"]:\n",
    "            token += ' '\n",
    "\n",
    "        final_sent += token\n",
    "        i += 1\n",
    "\n",
    "    final_sent = final_sent.replace(' ##', '')\n",
    "\n",
    "    return final_sent\n",
    "\n",
    "\n",
    "cap_model = BertForTokenClassification.from_pretrained(\n",
    "    \"ytu-ce-cosmos/turkish-base-bert-capitalization-correction\")\n",
    "punc_model = BertForTokenClassification.from_pretrained(\n",
    "    \"ytu-ce-cosmos/turkish-base-bert-punctuation-correction\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"ytu-ce-cosmos/turkish-base-bert-capitalization-correction\")\n",
    "\n",
    "\n",
    "capitalization_corr = pipeline(\"ner\", model=cap_model, tokenizer=tokenizer)\n",
    "punc_corr = pipeline(\"ner\", model=punc_model, tokenizer=tokenizer)\n",
    "\n",
    "sent = \"\"\"\n",
    "Çünkü benim geldiğim durağı görebilme şansım yok. Konum bazen yetişemiyor, yanlış oluyor, şoföre sürekli sormanız gerekiyor. Halbuki sesli sistem düzgün çalışsa hiçbir problem olmayacak. Veya minibüslerin tabelaları çok küçük, renksiz, ışıksız. Gerçekten anlayamıyoruz.\n",
    "\"\"\"\n",
    "\n",
    "print(end2end(sent, capitalization_corr, punc_corr))\n",
    "# Geçen hafta sonu arkadaşlarımla birlikte kısa bir tatile çıktık. Cuma akşamı yola çıktık. Yolculuk oldukça keyifli geçti. Cumartesi sabahı otele vardık. Odalarımıza yerleştikten sonra kahvaltıya indik. Kahvaltıda birçok seçenek vardı; omlet, simit, taze sıkılmış portakal suyu ve çeşitli peynirler. Kahvaltıdan sonra sahile gitmeye karar verdik. Deniz çok sakindi ve hava mükemmeldi. Denizde yüzdük, kumda yürüdük ve güneşlendik. Öğleden sonra şehri gezmeye çıktık. Tarihi yerleri ziyaret ettik ve bol bol fotoğraf çektik. Akşam yemeği için meşhur bir restorana gittik. Deniz ürünleri gerçekten çok tazeydi. Yemek sonrası otele döndüğümüzde çok yorgunduk ama tatilin ilk günü harika geçmişti. Pazar sabahı erken kalkıp bir doğa yürüyüşüne çıktık. Orman içinde yürümek çok huzur vericiydi. Dönüş yolunda biraz trafik vardı ama bu güzel tatilin ardından hiçbiri moralimizi bozamazdı. Eve vardığımızda herkes mutlu ve huzurluydu. Bir sonraki tatili planlamaya başladık bile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dördüncü dersimiz arkadaşlar ve eğlenceyi hayatından çıkarma. \n"
     ]
    }
   ],
   "source": [
    "sent = \"\"\"\n",
    "Dördüncü dersimiz, arkadaşlar ve eğlenceyi hayatından çıkarma\n",
    "\"\"\"\n",
    "\n",
    "print(end2end(sent, capitalization_corr, punc_corr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
